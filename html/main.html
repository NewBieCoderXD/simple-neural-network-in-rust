<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Making a neural network from scratch</title>
    <!-- <link rel="stylesheet" href="./style.css"> -->
    <style>
      @import url("https://fonts.cdnfonts.com/css/palatino-lt-black");
      /* blockquote {
        margin-top: 5px;
        margin-bottom: 5px;
      } */
      .multiline {
        display: flex;
        flex-direction: column;
        gap: 5px;
      }
      hr {
        background-color: blue;
        height: 3px;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      html {
        font-family: "Palatino LT Light", sans-serif;
      }
    </style>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <main>
      <h1>Math Explanation for Simple Neural Network</h1>
      By <a href="https://github.com/NewBieCoderXD">NewBieCoderXD</a>, repo:
      <a href="https://github.com/NewBieCoderXD/simple-neural-network-in-rust"
        >https://github.com/NewBieCoderXD/simple-neural-network-in-rust</a
      >
      Disclaimer: I'm, by no means, not a professional in computer science, just someone enthusiastic with calculus and Rust.<br>
      PLEASE take everything in this repo with a gain of salt. If you believe there is a mistake, please open issue on the github repo.

      <h2>Table of contents</h2>
      <ol>
        <li><a href="#neural-network">What is a neural network?</a></li>
        <li><a href="#notations">notations</a></li>
        <li><a href="#forward-propagation">Forward Propagation</a></li>
        <li><a href="#cost-function">Cost Function</a></li>
        <li><a href="#gradient-descent">Gradient Descent</a></li>
        <li>
          <a href="#back-propagation">Back Propagation</a>
          <ul>
            <li><a href="#weight-gradient">Weight Gradient</a></li>
            <li><a href="#bias-gradient">Bias Gradient</a></li>
            <li><a href="#error-1">Error of \(L\)-th Level</a></li>
            <li><a href="#error-2">Error of \(l\)-th Level</a></li>
          </ul>
        </li>
      </ol>
      <h2 id="neural-network">What is a neural network?</h2>
      A neural network is a model of how neurons work. Each layer of the network
      contains neurons, these neurons are connected to every neuron in its
      adjacent layer. <br />
      During learning or predicting, neurons will send real-number signal to
      every neuron in the layer on the right based on its value. Strength of
      signals are determined by "weights". After the right neurons receiving
      signals, it will transform the sum of signals using non-linear function
      called "activation function" to get activation values<br />
      To add more flexibility to model, we can add biases for each neuron. A
      bias is added to all signals that is sent to the neuron, unlike weight
      which only affects a connection of neurons <br />
      Bias is like intercept of linear regression, not adjusting it will lead to
      poor result. Let's represent this in mathematical terms <br />
      <ul style="margin: 0">
        <li>
          \(z_i^{[l]}\) means sum of signals multiplied by weights then added
          bias<br />
          vector/matrix form: \(z^{[l]}\)
        </li>
        <li>
          \(a_i^{[l]}\) means activation value of \(i\)-th neuron in \(l\)-th
          layer, we get from applying<br />
          vector/matrix form: \(a^{[l]}\)
        </li>
        <li>
          \(W_{ij}^{[l]}\) means weight of connection of ith neuron in \((l-1)\)
          th layer and \(j\)-th neuron in \(l\)-th layer<br />
          vector/matrix form: \(W^{[l]}\)
        </li>
        <li>
          \(b_i^{[l]}\) means bias value of ith neuron in \(l\)-th layer<br />
          vector/matrix form: \(b^{[l]}\)
        </li>
        <li>\(g^{[l]}(x)\) means activation function of \(l\)-th layer</li>
        <li>\(L\) means number of layers(both input,hidden,output)</li>
        <li>\(\hat y\) or \(a^{[L]}\) means predicted output</li>
      </ul>
      <b>Note:</b> \(i\)-th neuron is counted from top to bottom, while \(l\)-th
      layer is counted from left to right.

      <hr />

      <h2 id="forward-propagation">Forward Propagation</h2>
      Process of propagating input as a signal to get output. <br />
      When we predict output, we take input as a signal to the first layer(input
      layer), then propagate through hidden layers then the last layer(output
      layer), where activations of output layer is predicted output<br />
      Making an equation, we get \(\displaystyle{z_i^{[l]}=\sum_{j} W_{ij}^{[l]}
      \cdot a_j^{[l-1]} + b_i^{[l]}}\) and \(a_i^{[l]}=g^{[l]}(z_i^{[l]})\) We
      are basically amplifying signals from \(l-1\) layer by \(W_ij\), sum it
      then add bias to be a signal value for the right layer <br />
      To make things simpler for computers, let's generalize it to matrix
      notation. <br />
      Let's say that removing \(i\),\(j\) from notation will make it a matrix(or
      a vector). <br />
      <blockquote>
        \(\displaystyle{z^{[l]}=W^{[l]} \bullet a^{[l-1]} + b^{[l]}}\)
      </blockquote>
      and
      <blockquote>\(a^{[l]}=g^{[l]}(z^{[l]})\)</blockquote>

      <hr />

      <h2 id="cost-function">Cost Function</h2>

      <hr />

      <h2 id="gradient-descent">Gradient Descent</h2>
      We have our model that is basically auto regression, a model that can fits
      to almost any functions. But how do we train it?<br />
      The most popular way is to use gradient descent. The heart of gradient
      descent is that we want to minimize error.<br />
      To do that we can use calculus to find a local minima of error with
      respect to each parameter!<br />
      Let's say error is function \(J(x)\), we know that at local minima we will
      have first-order derivative of 0 and second-order derivative of
      positive.<br />
      But currently the slope isn't 0, how do we move it towards local
      minima?<br />
      Suppose current point is 1, while a point we move to be closer to local
      minima is 2<br />

      <blockquote>
        \[J^{\prime\prime}=\frac{d^2 J \left(x
        \right)}{dx^2}=\frac{J_2^\prime-J_1^\prime}{x_2-x_1}\]<br />
      </blockquote>
      We want \(J_2^\prime\) to be \(0\)<br />
      <blockquote>
        \[ \begin{align} \frac{d^2 J\left(x\right)}{dx^2}
        &=\frac{0-\frac{dJ\left(x\right)}{dx}}{x_2-x_1}\\ \left(x_2-x_1 \right)
        \frac{d^2 J \left( x \right)}{dx^2}&=-\frac{dJ \left(x \right)}{dx}\\
        x_2-x_1&=-\frac{dJ \left(x \right)}{dx} \frac{dx^2}{d^2 J \left(x
        \right)}\\ x_2&=x_1-\frac{dJ \left(x \right)}{dx} \cdot
        \frac{1}{J^{\prime\prime}} \end{align} \]
      </blockquote>
      or \[ \begin{align} x_{new}&=x_{old}-\frac{dJ \left(x \right)}{dx} \cdot
      \frac{1}{J^{\prime\prime}} \end{align} \]
      <br />
      Then we will get closer to local minima. This is essentially doing Newton
      method on first-order derivative.<br />
      But what is \(J^{\prime\prime}\)? We know that it must be positive, but
      what value?<br />
      We actually can choose if we want to moving towards minima fastly or
      slowly.<br />
      If \(J^{\prime\prime}\) is high, \(|x_2-x_1|\) is small, and vice
      versa.<br />
      But that seems counter-intuitive, let's define
      \(\alpha=\frac{1}{J^{\prime\prime}}\), and call it learning rate, so that
      if \(\alpha\) is high, \(|x_2-x_1|\) is high, and vice versa!<br />
      (Usually learning rate is around \(0.0001\) to \(0.1\))<br />
      So the equation becomes \[x_{new}=x_{old}-\alpha \cdot \frac{dJ(x)}{dx}\]

      <hr />

      <h2 id="back-propagation">Back Propagation</h2>
      Back propagation is a process of adjusting weights and biases, layer by
      layer, from right to left(propagate back) by using gradient descent.<br />
      Writing gradient descent of weights and biases into equations, we get
      <blockquote>
        \[W_{i,j (new)}^{[l]}=W_{i,j (old)}^{[l]}-\alpha \cdot \frac{\partial
        J(y,\hat y)}{\partial W_{i,j}^{[l]}}\]
      </blockquote>
      <blockquote>
        \[b_{i (new)}^{[l]}=b_{i (old)}^{[l]}-\alpha \cdot \frac{\partial
        J(y,\hat y)}{\partial b_i^{[l]}}\]
      </blockquote>

      We need to find weight gradient(\(\frac{dJ(y,\hat y)}{dW_{i,j}^{[l]}}\))
      and bias gradient(\(\frac{\partial J(y,\hat y)}{\partial b_i^{[l]}}\)).

      <h4 id="weight-gradient">• Weight Gradient</h4>
      using chain rule, we get \[\frac{\partial J(y,\hat y)}{\partial
      W_{i,j}^{[l]}}=\frac{\partial J(y,\hat y)}{\partial z_{j}^{[l]}}
      \frac{\partial z_{j}^{[l]}}{\partial W_{i,j}^{[l]}}\] <br />
      Why? because \(W_{i,j}^{[l]}\) only affects \(z_{j}^{[l]}\) <br />
      We know that \[ \begin{align} \frac{\partial z_{j}^{[l]}}{\partial
      W_{i,j}^{[l]}}&=a_i^{[l-1]} \\ \frac{\partial J(y,\hat y)}{\partial
      W_{i,j}^{[l]}}&=\frac{\partial J(y,\hat y)}{\partial z_{j}^{[l]}}
      a_i^{[l-1]} \end{align} \]

      <h4 id="bias-gradient">• Bias Gradient</h4>
      using chain rule, we get \[\frac{\partial J(y,\hat y)}{\partial
      b_i^{[l]}}=\frac{\partial J(y,\hat y)}{\partial z_{i}^{[l]}}
      \frac{\partial z_{i}^{[l]}}{\partial b_{i}^{[l]}}\] <br />
      Why? because \(b_{i}^{[l]}\) only affects \(z_{i}^{[l]}\) <br />
      \[ \begin{align} \frac{\partial J(y,\hat y)}{\partial
      b_{i}^{[l]}}&=\frac{\partial J(y,\hat y)}{\partial z_{j}^{[l]}}
      \frac{\partial (\sum W_{i,j}^{[l]} \cdot a_i^{[l-1]}+b_i^{[l]})}{\partial
      b_{i}^{[l]}}\\ \frac{\partial J(y,\hat y)}{\partial
      b_{i}^{[l]}}&=\frac{\partial J(y,\hat y)}{\partial z_{j}^{[l]}}
      \end{align} \] We can see that weight gradient and bias gradient share the
      same term, let's name it \(\delta_j^{[l]}\) (delta), it's basically error
      of \(j\)-th neuron in \(l\)-th layer.<br />
      \[\delta_j^{[l]} = \frac{\partial J(y,\hat y)}{\partial z_{j}^{[l]}}\] Or
      vector form \[\delta^{[l]} = \frac{\partial J(y,\hat y)}{\partial z^{[l]}}
      \] we can write \[ \begin{align} \frac{\partial J(y,\hat y)}{\partial
      W_{i,j}^{[l]}}&=\delta_j^{[l]} a_i^{[l-1]} \\ \frac{\partial J(y,\hat
      y)}{\partial W_{i,j}^{[l]}}&=\delta_j^{[l]} a_i^{[l-1]} \end{align} \] \[
      \nabla_{W^{[l]}} J(y,\hat y) = \begin{bmatrix} a_0^{[l-1]} \delta_0^{[l]}
      & a_1^{[l-1]} \delta_0^{[l]} & a_2^{[l-1]} \delta_0^{[l]} & ... \\
      a_0^{[l-1]} \delta_1^{[l]} & a_1^{[l-1]} \delta_1^{[l]} & a_2^{[l-1]}
      \delta_1^{[l]}& ... \\ a_0^{[l-1]} \delta_2^{[l]} & a_1^{[l-1]}
      \delta_2^{[l]} & a_2^{[l-1]} \delta_2^{[l]}& ... \\ ... & ... & ... & ...
      \\ \end{bmatrix} \] \[ \nabla_{W^{[l]}} J(y,\hat y) = \delta^{[l]}
      a^{[l-1]} \] and \[ \begin{align} \frac{\partial J(y,\hat y)}{\partial
      b_{i}^{[l]}}=\delta_i^{[l]}\\ \frac{\partial J(y,\hat y)}{\partial
      b^{[l]}}=\delta^{[l]} \end{align} \] now if we can find
      \(\delta^{[l]}\) we are done.

      <h4 id="error-1">• Error of \(L\)-th Level</h4>
      We need to use chain rule to introduce \(a^{[L]}\) because it is direct
      argument of cost functions.
      <blockquote>
        \[ \begin{align} \frac{\partial J(y,\hat y)}{\partial
        z_{j}^{[L]}}&=\frac{\partial J(y,\hat y)}{\partial a_{j}^{[L]}} \cdot
        \frac{\partial a_{j}^{[L]}}{\partial z_{j}^{[L]}} \\ \frac{\partial
        J(y,\hat y)}{\partial z_{j}^{[L]}}&=\frac{\partial J(y,\hat y)}{\partial
        a_{j}^{[L]}} \cdot g^{\prime [L]} \bigl(z_{j}^{[L]} \bigr) \\
        \frac{\partial J(y,\hat y)}{\partial z_{j}^{[L]}}&=\frac{\partial
        J(y,\hat y)}{\partial a_{j}^{[L]}} \cdot g^{\prime [L]}
        \bigl(z_{j}^{[L]} \bigr) \end{align}\]
      </blockquote>
      vector form:<br />
      <blockquote>
        \[\frac{\partial J(y,\hat y)}{\partial z^{[L]}}=\frac{\partial J(y,\hat
        y)}{\partial a^{[L]}} \cdot g^{\prime [L]} \bigl(z^{[L]} \bigr)\]
      </blockquote>
      or
      <blockquote>
        \[\nabla_{z^{[L]}} J(y,\hat y)=\nabla_{a^{[L]}} J(y,\hat y) \cdot
        g^{\prime [L]} \bigl(z^{[L]} \bigr)\]
      </blockquote>

      <h4 id="error-2">• Error of \(l\)-th Level</h4>
      <!-- prettier-ignore -->
      <blockquote>
        \[ \begin{align} 
        \delta_i^{[l]} &= \frac{\partial J(y,\hat y)}{\partial z_i^{[l]}} \\ 
        &= \sum_{j} \frac{\partial J(y,\hat y)}{\partial
        z_j^{[l+1]}} \cdot \frac{\partial z_j^{[l+1]}}{\partial z_i^{[l]}} \\ 
        &= \sum_{j} \delta_j^{[l+1]} \cdot \frac{\partial \left( \sum_k W_{kj}^{[l+1]} \cdot a_k^{[l]} + b_j^{[l+1]} \right)}{\partial z_i^{[l]}} \\ 
        &= \sum_{j} \delta_j^{[l+1]} \cdot \frac{\partial \left( \sum_k W_{kj}^{[l+1]} \cdot a_k^{[l]} + b_j^{[l+1]} \right)}{\partial a_i^{[l]}} \cdot \frac{\partial a_i^{[l]}}{\partial z_i^{[l]}} \\
        &= \sum_{j} \delta_j^{[l+1]} \cdot W_{ij}^{[l+1]} \cdot g^{\prime [l]} \bigl(z_i^{[l]} \bigr) \\
        &=\begin{bmatrix}
          g^{\prime [l]} \bigl(z_0^{[l]} \bigr) \\
          g^{\prime [l]} \bigl(z_1^{[l]} \bigr) \\
          ...
        \end{bmatrix} \odot
        \begin{bmatrix}
          \sum_{j} \delta_j^{[l+1]} \cdot W_{0,j}^{[l+1]} \\
          \sum_{j} \delta_j^{[l+1]} \cdot W_{1,j}^{[l+1]} \\
          ...
        \end{bmatrix} \\
        &=\begin{bmatrix}
          g^{\prime [l]} \bigl(z_0^{[l]} \bigr) \\
          g^{\prime [l]} \bigl(z_1^{[l]} \bigr) \\
          ...
        \end{bmatrix} \odot
        \begin{bmatrix}
          W_{0,0}^{[l+1]} &  W_{0,1}^{[l+1]} & ... \\
          W_{1,0}^{[l+1]} &  W_{1,1}^{[l+1]} & ... \\
          ... & ... & ...
        \end{bmatrix} \cdot
        \begin{bmatrix}
          \delta_j^{[l+1]} \\
          \delta_j^{[l+1]} \\
          ...
        \end{bmatrix} \\
        &=g^{\prime [l]} \bigl(z^{[l]} \bigr) \odot \left (W^{[l+1]} \right)^T \cdot \delta^{[l+1]} \\
        \delta^{[l-1]} &=g^{\prime [l-1]} \bigl(z^{[l-1]} \bigr) \odot \left (W^{[l]} \right)^T \cdot \delta^{[l]}
        \end{align} \]
      </blockquote>
    </main>
  </body>
</html>
