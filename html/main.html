<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Making a neural network from scratch</title>
    <!-- <link rel="stylesheet" Zhref="./style.css"> -->
    <style>
      blockquote {
        margin-top: 5px;
        margin-bottom: 5px;
      }
    </style>
    <script
      type="text/javascript"
      src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js"
    ></script>
  </head>
  <body>
    <main>
      <h1>What is a neural network?</h1>
      A neural network is a model of how neurons work. Each layer of the network
      contains neurons, these neurons are connected to every neuron in its
      adjacent layer. <br />
      During learning or predicting, neurons will send real-number signal to
      every neuron in the layer on the right based on its value. Strength of
      signals are determined by "weights". After the right neurons receiving
      signals, it will transform the sum of signals using non-linear function
      called "activation function" to get activation values<br />
      To add more flexibility to model, we can add biases for each neuron. A
      bias is added to all signals that is sent to the neuron, unlike weight
      which only affects a connection of neurons <br />
      Bias is like intercept of linear regression, not adjusting it will lead to
      poor result. Let's represent this in mathematical terms <br />
      <ul style="margin:0">
        <li>
          $z_i^{[l]}$ means sum of signals multiplied by weights then added bias
        </li>
        <li>
          $a_i^{[l]}$ means activation value of $i$-th neuron in $l$-th layer,
          we get from applying
        </li>
        <li>
          $w_ij^{[l]}$ means weight of connection of ith neuron in $(l-1)$ th
          layer and $j$-th neuron in $l$-th layer
        </li>
        <li>$b_i^{[l]}$ means bias value of ith neuron in $l$-th layer</li>
        <li>$g^{[l]}(x)$ means activation function of $l$-th layer</li>
      </ul>
      <b>Note:</b> $i$-th neuron is counted from top to bottom, while $l$-th
      layer is counted from left to right.
      <h1>Forward Propagation</h1>
      Process of propagating input as a signal to get output. <br />
      When we predict output, we take input as a signal to the first layer(input
      layer), then propagate through hidden layers then the last layer(output
      layer), where activations of output layer is predicted output<br />
      Making an equation, we get
      <blockquote>
        $\displaystyle{z_i^{[l]}=\sum_{j} W_{ij}^{[l]} \cdot a_j^{[l-1]} +
        b_i^{[l]}}$
      </blockquote>
      and
      <blockquote>$a_i^{[l]}=g^{[l]}(z_i^{[l]})$</blockquote>
      We are basically amplifying signals from $l-1$ layer by $W_ij$, sum it
      then add bias to be a signal value for the right layer <br />
      To make things simpler for computers, let's generalize it to matrix
      notation. <br />
      Let's say that removing $i$,$j$ from notation will make it a matrix(or a
      vector). <br />
      <blockquote>
        $\displaystyle{z^{[l]}=W^{[l]} \bullet a^{[l-1]} + b^{[l]}}$
      </blockquote>
      and
      <blockquote>$a^{[l]}=g^{[l]}(z^{[l]})$</blockquote>
    </main>

    <!-- <script src="index.js"></script> -->
  </body>
</html>
